{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "\n",
    "import os.path as pth\n",
    "import matplotlib.pyplot as plt # Matplotlib v3.5.1\n",
    "import numpy as np # Numpy v1.23.4\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils import set_rcParams\n",
    "\n",
    "set_rcParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json file containing clusters informations\n",
    "\n",
    "with open('clusters_informations_for_hybrid_simulations.json') as json_file:\n",
    "    clusters_infos =  json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to read .mat results files\n",
    "\n",
    "def generate_reduced_network_node_id_dict(clustered_nodes: list[int]) -> dict:\n",
    "    \"\"\"Generates dictionary to identify the remaining nodes from the original network nodes\n",
    "\n",
    "    Args:\n",
    "        clustered_nodes (list[int]): list of clustered nodes (in julia indexing)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary to associate the original node id to the reduced network node ids\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    dict_ = {}\n",
    "    for i in range(71): # There is 71 nodes in the case study network 1\n",
    "        if (i+1) in clustered_nodes:\n",
    "            step += 1\n",
    "        dict_[i] = i-step\n",
    "    return dict_\n",
    "\n",
    "def prepocessing(dict_physical_values) -> dict:\n",
    "    \"\"\"Preprocess the read data from .mat files\n",
    "\n",
    "    Args:\n",
    "        dict_physical_values (dict): the dictionary from the mat file\n",
    "\n",
    "    Returns:\n",
    "        dict: the improved and treated dict file\n",
    "    \"\"\"\n",
    "    dict_ = {}\n",
    "    # Some prepocessing\n",
    "    first_dynamic_step_interest = 10*60\n",
    "    last_dynamic_steps_horizon = first_dynamic_step_interest + 2*7*24*60 - 1 # 1 week only for minute-time steps\n",
    "\n",
    "    for physical_state_key in dict_physical_values:\n",
    "        array = np.array(dict_physical_values[physical_state_key])\n",
    "        if physical_state_key in ['trcs', 'mc']: # We take all for those\n",
    "            dict_[physical_state_key] = array\n",
    "        else:\n",
    "            dict_[physical_state_key] = array[first_dynamic_step_interest:last_dynamic_steps_horizon,:]\n",
    "    return dict_\n",
    "\n",
    "def compute_mae_with_interval(distribution) -> tuple:\n",
    "    \"\"\"Computes the mean with interval confidence of 98%\n",
    "\n",
    "    Args:\n",
    "        distribution (numpy): the distribution\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean, +- confidence interval)\n",
    "    \"\"\"\n",
    "    return np.mean(distribution), 1.96*np.std(distribution)/np.sqrt(len(distribution))\n",
    "\n",
    "def get_hybrid_simulation_performances(scenario_dict_values: dict, model_key: str, clusters_keys:  Union[list[str],str], cluster_key_result: str = None, sources_to_consider=[0,56]) -> dict:\n",
    "    \"\"\"Reads the hybrid results and assess the reduced network simulation performances\n",
    "\n",
    "    Args:\n",
    "        scenario_dict_values (dict): the dictionary containing the scenario results including full physical and reduced networks\n",
    "        model_key (str): the model considered [rnn, cnn]\n",
    "        clusters_keys (Union[list[str],str]): the clusters keys\n",
    "        cluster_key_result (str, optional): clusters key used for hybrid results name. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        Exception: if cluster key not well defined\n",
    "\n",
    "    Returns:\n",
    "        dict: the dictionary of performances\n",
    "    \"\"\"\n",
    "    incoming_pipes_ids = []\n",
    "    incoming_pipes_ids_trout_nodes = []\n",
    "    \n",
    "    outgoing_pipes_ids = []\n",
    "    outgoing_pipes_ids_tsout_nodes = []\n",
    "    \n",
    "    clustered_nodes = []\n",
    "    associated_node_ids = dict()\n",
    "    perfs = dict()\n",
    "    \n",
    "    if type(clusters_keys) == list:\n",
    "        if cluster_key_result == None:\n",
    "            raise Exception('Cluster key result must be defined for many clusters considered')\n",
    "        for key in clusters_keys:\n",
    "            cluster_info = clusters_infos[f'cluster_{key}']\n",
    "            # print(cluster_info)\n",
    "            incoming_pipes_ids.extend(cluster_info['incoming_pipes_id'])\n",
    "            incoming_pipes_ids_trout_nodes.extend(cluster_info['incoming_pipes_remixing_node_id'])\n",
    "            outgoing_pipes_ids.extend(cluster_info['outgoing_pipe_id'])\n",
    "            outgoing_pipes_ids_tsout_nodes.extend(cluster_info['outgoing_pipe_remixing_node_id'])\n",
    "            clustered_nodes.extend(cluster_info['cluster_nodes'])\n",
    "\n",
    "        associated_node_ids = generate_reduced_network_node_id_dict(clustered_nodes)\n",
    "        key = cluster_key_result # key to identify dict of results\n",
    "        \n",
    "    else:\n",
    "        key = clusters_keys\n",
    "        cluster_info = clusters_infos[f'cluster_{key}']\n",
    "        # print(cluster_info)\n",
    "        incoming_pipes_ids.extend(cluster_info['incoming_pipes_id'])\n",
    "        incoming_pipes_ids_trout_nodes.extend(cluster_info['incoming_pipes_remixing_node_id'])\n",
    "        outgoing_pipes_ids.extend(cluster_info['outgoing_pipe_id'])\n",
    "        outgoing_pipes_ids_tsout_nodes.extend(cluster_info['outgoing_pipe_remixing_node_id'])\n",
    "        clustered_nodes.extend(cluster_info['cluster_nodes'])\n",
    "        associated_node_ids = generate_reduced_network_node_id_dict(cluster_info['cluster_nodes'])\n",
    "\n",
    "    perfs['key'] = key\n",
    "    perfs['type'] = cluster_info['cluster_type']\n",
    "    perfs['clustered_nodes_number'] = len(clustered_nodes)\n",
    "    perfs['clustered_nodes_percentage'] = 100*(len(clustered_nodes) / 71)\n",
    "    perfs['clustered_pipes_percentage'] = 100*((len(clustered_nodes)+len(outgoing_pipes_ids)) / 71) # Tree-like assumption => number of nodes == number edges\n",
    "    \n",
    "    loads_mare_f = []\n",
    "    loads_incoming_mare_f = []\n",
    "    loads_outgoing_mare_f = []\n",
    "    \n",
    "    incoming_loads = 0\n",
    "    incoming_mws = 0\n",
    "    for i, el in enumerate(incoming_pipes_ids):\n",
    "        el_i = incoming_pipes_ids_trout_nodes[i]\n",
    "        pipe_index = int(el)-1 # Ingoing pipe index in the original network (needed for effective comparison) !! -1 as julia are indexed from 1\n",
    "        node_index = int(el_i)-1 # Adjacent nodes to the cluster (in the return direction) !! -1 as julia are indexed from 1\n",
    "\n",
    "        mw = np.abs(np.mean(scenario_dict_values['full_physic']['mw'][:,pipe_index]))\n",
    "        # print(f'Original incoming pipe (scenario) {el} traversed by mass flow rates with nominal value of {mw:.2f} Kg/s')\n",
    "\n",
    "        tr_node_mix = scenario_dict_values['full_physic']['tr'][:,node_index]\n",
    "        ts_node_mix = scenario_dict_values['full_physic']['ts'][:,node_index]\n",
    "            \n",
    "        tr_node_mix_reduced = scenario_dict_values[model_key][key]['tr'][:,associated_node_ids[node_index]]\n",
    "        ts_node_mix_reduced = scenario_dict_values[model_key][key]['ts'][:,associated_node_ids[node_index]]\n",
    "        \n",
    "        pw_adj_node = mw*4200*(ts_node_mix-tr_node_mix)\n",
    "        pw_adj_node_reduced = mw*4200*(ts_node_mix_reduced-tr_node_mix_reduced)\n",
    "        \n",
    "        loads_mare_f.extend(100*(np.abs(1 - pw_adj_node_reduced/pw_adj_node)))\n",
    "        \n",
    "        incoming_loads += np.mean(np.abs(pw_adj_node))\n",
    "        incoming_mws += np.mean(np.abs(mw)) \n",
    "    \n",
    "    outgoing_loads = 0\n",
    "    outgoing_mws = 0\n",
    "    for i, el in enumerate(outgoing_pipes_ids):\n",
    "        el_i = outgoing_pipes_ids_tsout_nodes[i]\n",
    "        pipe_index = int(el)-1\n",
    "        node_index = int(el_i)-1\n",
    "        \n",
    "        mw = np.abs(np.mean(scenario_dict_values['full_physic']['mw'][:,pipe_index]))\n",
    "\n",
    "        tr_node_mix = scenario_dict_values['full_physic']['tr'][:,node_index]\n",
    "        ts_node_mix = scenario_dict_values['full_physic']['ts'][:,node_index]\n",
    "        tr_node_mix_reduced = scenario_dict_values[model_key][key]['tr'][:,associated_node_ids[node_index]]\n",
    "        ts_node_mix_reduced = scenario_dict_values[model_key][key]['ts'][:,associated_node_ids[node_index]]\n",
    "        \n",
    "        pw_adj_node = mw*4200*(ts_node_mix-tr_node_mix)\n",
    "        pw_adj_node_reduced = mw*4200*(ts_node_mix_reduced-tr_node_mix_reduced)\n",
    "        \n",
    "        loads_mare_f.extend(100*(np.abs(1 - pw_adj_node_reduced/pw_adj_node)))\n",
    "        \n",
    "        outgoing_loads += np.mean(np.abs(pw_adj_node))\n",
    "        outgoing_mws += np.mean(np.abs(mw)) \n",
    "        \n",
    "    mare_f, conf_f = compute_mae_with_interval(loads_mare_f)\n",
    "    \n",
    "    perfs['incoming_loads'] = incoming_loads\n",
    "    perfs['outgoing_loads'] = outgoing_loads\n",
    "    perfs['incoming_mws'] = incoming_mws\n",
    "    perfs['outgoing_mws'] = outgoing_mws\n",
    "    perfs['Thermal loads (MARE)'] = mare_f\n",
    "    perfs['Thermal loads (MARE-Conf_interval)'] = conf_f\n",
    "    perfs['Thermal loads (errors)'] = loads_mare_f\n",
    "    \n",
    "    mare_f, conf_f = compute_mae_with_interval(loads_mare_f)\n",
    "    mare_f_in, conf_f_in = compute_mae_with_interval(loads_incoming_mare_f)\n",
    "    mare_f_out, conf_f_out = compute_mae_with_interval(loads_outgoing_mare_f)\n",
    "    \n",
    "    perfs['Thermal loads incoming (MARE)'] = mare_f_in\n",
    "    perfs['Thermal loads incoming (MARE-Conf_interval)'] = conf_f_in\n",
    "    perfs['Thermal loads outgoing (MARE)'] = mare_f_out\n",
    "    perfs['Thermal loads outgoing (MARE-Conf_interval)'] = conf_f_out\n",
    "    \n",
    "    perfs['Thermal loads (MARE)'] = mare_f\n",
    "    perfs['Thermal loads (MARE-Conf_interval)'] = conf_f\n",
    "    perfs['Thermal loads (errors)'] = loads_mare_f\n",
    "        \n",
    "    tr_node_source_f = scenario_dict_values['full_physic']['tr'][:,sources_to_consider].reshape(1,-1)\n",
    "    ts_node_source_f = scenario_dict_values['full_physic']['ts'][:,sources_to_consider].reshape(1,-1)\n",
    "    \n",
    "    associated_sources_nodes_ids = [associated_node_ids[i] for i in sources_to_consider]\n",
    "    tr_node_source_reduced_f = scenario_dict_values[model_key][key]['tr'][:,associated_sources_nodes_ids].reshape(1,-1)\n",
    "    ts_node_source_reduced_f = scenario_dict_values[model_key][key]['ts'][:,associated_sources_nodes_ids].reshape(1,-1)\n",
    "\n",
    "    mw_source_node_f = scenario_dict_values['full_physic']['mw'][:,sources_to_consider].reshape(1,-1)\n",
    "    \n",
    "    pw_adj_node_f = mw_source_node_f*4200*(ts_node_source_f-tr_node_source_f)\n",
    "    pw_adj_node_reduced_f = mw_source_node_f*4200*(ts_node_source_reduced_f-tr_node_source_reduced_f)\n",
    "    \n",
    "    mae_r_f, conf_r_f = compute_mae_with_interval(np.abs(tr_node_source_f-tr_node_source_reduced_f))\n",
    "    # print(f'Source return temperature conservation (scenario) MAE = {mae_r_f:.4f} °C +- {conf_r_f:.4f}') \n",
    "    perfs['Source return temp (MAE)'] = mae_r_f\n",
    "    perfs['Source return temp (MAE-Conf_interval)'] = conf_r_f\n",
    "    perfs['Source return temp (errors)'] = np.abs(tr_node_source_f-tr_node_source_reduced_f)\n",
    "\n",
    "    mare_pw_f, conf_pw_f = compute_mae_with_interval(100*np.abs(1 - pw_adj_node_reduced_f/pw_adj_node_f))\n",
    "    # print(f'Source generation power conservation (scenario) MARE = {mare_pw_f:.4f} % +- {conf_pw_f:.4f}') \n",
    "    perfs['Source gen (MARE)'] = mare_pw_f\n",
    "    perfs['Source gen (MARE-Conf_interval)'] = conf_pw_f\n",
    "    perfs['Source gen (errors)'] = 100*np.abs(1 - pw_adj_node_reduced_f/pw_adj_node_f)\n",
    "    \n",
    "    return perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d22rodri\\Desktop\\WORKS\\1st_article_folders_git\\ML_clusters\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\d22rodri\\Desktop\\WORKS\\1st_article_folders_git\\ML_clusters\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\d22rodri\\Desktop\\WORKS\\1st_article_folders_git\\ML_clusters\\lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\d22rodri\\Desktop\\WORKS\\1st_article_folders_git\\ML_clusters\\lib\\site-packages\\numpy\\core\\_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "c:\\Users\\d22rodri\\Desktop\\WORKS\\1st_article_folders_git\\ML_clusters\\lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_g_scenario_1_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_manual_1_scenario_1_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_g_manual_1_scenario_1_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_i_scenario_1_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_m_scenario_1_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1 \n",
    "\n",
    "folder = os.path.join('ARTICLE_hybrid_results')\n",
    "first_scenario_results = {}\n",
    "# Note: non present values are extrapolated\n",
    "# Les demandes ne sont pas modifiés trop dont utiliser l'autre \n",
    "# 1st scenario == ts var mais demandes differentes\n",
    "\n",
    "# I call it large clusters because I create them for the non present clusters at the begining\n",
    "# but I end up redoing some \"normal\" clusters leading v2 results \n",
    "# similar to v1 but I am afraid that some values might have change so compare physical v2 with reduced v2 to obtain the results\n",
    "\n",
    "full_network_physical_file_path = os.path.join(folder, 'original_large_clusters_scenario_1.mat')\n",
    "first_scenario_results['full_physic'] = h5py.File(full_network_physical_file_path, 'r') # Dict ['load', 'mc', 'mw', 'topology_load', 'tr', 'trcs', 'trin', 'trout', 'ts', 'tsin', 'tsout']\n",
    "first_scenario_results['full_physic'] = prepocessing(first_scenario_results['full_physic'])\n",
    "\n",
    "first_scenario_results['rnn'] = {}\n",
    "first_scenario_results['cnn'] = {}\n",
    "\n",
    "perfs_per_models = {}\n",
    "perfs_per_models['rnn'] = {}\n",
    "perfs_per_models['cnn'] = {}\n",
    "\n",
    "folder_scenario_1 = 'scenario_1_results'\n",
    "if not os.path.isdir(folder_scenario_1):\n",
    "    os.mkdir(folder_scenario_1)\n",
    "\n",
    "import json\n",
    "import pickle \n",
    "\n",
    "with open(os.path.join(folder_scenario_1, f'results_full_network_v2.pkl'), 'wb') as f:\n",
    "    pickle.dump(first_scenario_results['full_physic'], f)\n",
    "\n",
    "for key in ['g', 'manual_1', 'g_manual_1', 'i', 'm', 'f_manual_1']:\n",
    "    for model in ['rnn', 'cnn']:\n",
    "        try:\n",
    "            reduced_file_path_rnn = os.path.join(folder, f'hybrid_{key}_scenario_1_model_{model}.mat')\n",
    "            dict_values = h5py.File(reduced_file_path_rnn, 'r')\n",
    "            first_scenario_results[model][key] = prepocessing(dict_values)\n",
    "            if key == 'f_manual_1':\n",
    "                sources = [0,56] # can add '56'\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(first_scenario_results, model, ['f','manual_1'], cluster_key_result='f_manual_1', sources_to_consider=sources)\n",
    "            elif key == 'g_manual_1':\n",
    "                sources = [0,56] # can add '56'\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(first_scenario_results, model, ['g','manual_1'], cluster_key_result='g_manual_1', sources_to_consider=sources)\n",
    "            else:\n",
    "                sources = [0] # can add '56'\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(first_scenario_results, model, key, cluster_key_result=key, sources_to_consider=sources)\n",
    "            \n",
    "            with open(os.path.join(folder_scenario_1, f'results_{model}_{key}_v2.pkl'), 'wb') as f:\n",
    "                pickle.dump(perfs_per_models[model][key], f)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_manual_1_scenario_2_model_rnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_manual_1_scenario_2_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_g_manual_1_scenario_2_model_rnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_g_manual_1_scenario_2_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "[Errno 2] Unable to synchronously open file (unable to open file: name = 'ARTICLE_hybrid_results\\hybrid_f_manual_1_scenario_2_model_cnn.mat', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# Scenario 2\n",
    "\n",
    "folder = os.path.join('ARTICLE_hybrid_results')\n",
    "second_scenario_results = {}\n",
    "# Note: non present values are extrapolated\n",
    "# Les demandes ne sont pas modifiés trop dont utiliser l'autre \n",
    "# 1st scenario == ts var mais demandes differentes\n",
    "\n",
    "full_network_physical_file_path = os.path.join(folder, 'original_large_clusters_scenario_2.mat')\n",
    "second_scenario_results['full_physic'] = h5py.File(full_network_physical_file_path, 'r') # Dict ['load', 'mc', 'mw', 'topology_load', 'tr', 'trcs', 'trin', 'trout', 'ts', 'tsin', 'tsout']\n",
    "second_scenario_results['full_physic'] = prepocessing(second_scenario_results['full_physic'])\n",
    "\n",
    "second_scenario_results['rnn'] = {}\n",
    "second_scenario_results['cnn'] = {}\n",
    "\n",
    "perfs_per_models = {}\n",
    "perfs_per_models['rnn'] = {}\n",
    "perfs_per_models['cnn'] = {}\n",
    "\n",
    "folder_scenario_2 = 'scenario_2_results'\n",
    "if not os.path.isdir(folder_scenario_2):\n",
    "    os.mkdir(folder_scenario_2)\n",
    "\n",
    "import json\n",
    "import pickle \n",
    "\n",
    "with open(os.path.join(folder_scenario_2, f'results_full_network_v2.pkl'), 'wb') as f:\n",
    "    pickle.dump(second_scenario_results['full_physic'], f)\n",
    "\n",
    "for key in ['g', \"e\", 'manual_1', 'g_manual_1', 'i', 'm', 'f_manual_1']:\n",
    "    for model in ['rnn', 'cnn']:\n",
    "        try:\n",
    "            reduced_file_path_rnn = os.path.join(folder, f'hybrid_{key}_scenario_2_model_{model}.mat')\n",
    "            dict_values = h5py.File(reduced_file_path_rnn, 'r')\n",
    "            second_scenario_results[model][key] = prepocessing(dict_values)\n",
    "            if key == 'f_manual_1':\n",
    "                sources = [0,56] # can add '56'\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(second_scenario_results, model, ['f','manual_1'], cluster_key_result='f_manual_1', sources_to_consider=sources)\n",
    "            elif key == 'g_manual_1':\n",
    "                sources = [0,56] # can add '56'\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(second_scenario_results, model, ['g','manual_1'], cluster_key_result='g_manual_1', sources_to_consider=sources)\n",
    "            else:\n",
    "                sources = [0] # can add '56'\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(second_scenario_results, model, key, cluster_key_result=key, sources_to_consider=sources)\n",
    "            \n",
    "            with open(os.path.join(folder_scenario_2, f'results_{model}_{key}_v2.pkl'), 'wb') as f:\n",
    "                pickle.dump(perfs_per_models[model][key], f)\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_clusters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
