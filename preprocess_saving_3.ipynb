{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "\n",
    "import os.path as pth\n",
    "import matplotlib.pyplot as plt # Matplotlib v3.5.1\n",
    "import numpy as np # Numpy v1.23.4\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils import set_rcParams\n",
    "\n",
    "set_rcParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json file containing clusters informations\n",
    "\n",
    "with open('clusters_informations_for_hybrid_simulations.json') as json_file:\n",
    "    clusters_infos =  json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to read .mat results files\n",
    "\n",
    "def generate_reduced_network_node_id_dict(clustered_nodes: list[int]) -> dict:\n",
    "    \"\"\"Generates dictionary to identify the remaining nodes from the original network nodes\n",
    "\n",
    "    Args:\n",
    "        clustered_nodes (list[int]): list of clustered nodes (in julia indexing)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary to associate the original node id to the reduced network node ids\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    dict_ = {}\n",
    "    for i in range(71): # There is 71 nodes in the case study network 1\n",
    "        if (i+1) in clustered_nodes:\n",
    "            step += 1\n",
    "        dict_[i] = i-step\n",
    "    return dict_\n",
    "\n",
    "def prepocessing(dict_physical_values) -> dict:\n",
    "    \"\"\"Preprocess the read data from .mat files\n",
    "\n",
    "    Args:\n",
    "        dict_physical_values (dict): the dictionary from the mat file\n",
    "\n",
    "    Returns:\n",
    "        dict: the improved and treated dict file\n",
    "    \"\"\"\n",
    "    dict_ = {}\n",
    "    # Some prepocessing\n",
    "    first_dynamic_step_interest = 10*60\n",
    "    last_dynamic_steps_horizon = first_dynamic_step_interest + 2*7*24*60 - 1 # 1 week only for minute-time steps\n",
    "\n",
    "    for physical_state_key in dict_physical_values:\n",
    "        array = np.array(dict_physical_values[physical_state_key])\n",
    "        if physical_state_key in ['trcs', 'mc']: # We take all for those\n",
    "            dict_[physical_state_key] = array\n",
    "        else:\n",
    "            dict_[physical_state_key] = array[first_dynamic_step_interest:last_dynamic_steps_horizon,:]\n",
    "    return dict_\n",
    "\n",
    "def compute_mae_with_interval(distribution) -> tuple:\n",
    "    \"\"\"Computes the mean with interval confidence of 98%\n",
    "\n",
    "    Args:\n",
    "        distribution (numpy): the distribution\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean, +- confidence interval)\n",
    "    \"\"\"\n",
    "    return np.mean(distribution), 1.96*np.std(distribution)/np.sqrt(len(distribution))\n",
    "\n",
    "def get_hybrid_simulation_performances(scenario_dict_values: dict, model_key: str, clusters_keys:  Union[list[str],str], cluster_key_result: str = None, sources_to_consider=[0,56]) -> dict:\n",
    "    \"\"\"Reads the hybrid results and assess the reduced network simulation performances\n",
    "\n",
    "    Args:\n",
    "        scenario_dict_values (dict): the dictionary containing the scenario results including full physical and reduced networks\n",
    "        model_key (str): the model considered [rnn, cnn]\n",
    "        clusters_keys (Union[list[str],str]): the clusters keys\n",
    "        cluster_key_result (str, optional): clusters key used for hybrid results name. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        Exception: if cluster key not well defined\n",
    "\n",
    "    Returns:\n",
    "        dict: the dictionary of performances\n",
    "    \"\"\"\n",
    "    incoming_pipes_ids = []\n",
    "    incoming_pipes_ids_trout_nodes = []\n",
    "    \n",
    "    outgoing_pipes_ids = []\n",
    "    outgoing_pipes_ids_tsout_nodes = []\n",
    "    \n",
    "    clustered_nodes = []\n",
    "    associated_node_ids = dict()\n",
    "    perfs = dict()\n",
    "    \n",
    "    if type(clusters_keys) == list:\n",
    "        if cluster_key_result == None:\n",
    "            raise Exception('Cluster key result must be defined for many clusters considered')\n",
    "        for key in clusters_keys:\n",
    "            cluster_info = clusters_infos[f'cluster_{key}']\n",
    "            # print(cluster_info)\n",
    "            incoming_pipes_ids.extend(cluster_info['incoming_pipes_id'])\n",
    "            incoming_pipes_ids_trout_nodes.extend(cluster_info['incoming_pipes_remixing_node_id'])\n",
    "            outgoing_pipes_ids.extend(cluster_info['outgoing_pipe_id'])\n",
    "            outgoing_pipes_ids_tsout_nodes.extend(cluster_info['outgoing_pipe_remixing_node_id'])\n",
    "            clustered_nodes.extend(cluster_info['cluster_nodes'])\n",
    "\n",
    "        associated_node_ids = generate_reduced_network_node_id_dict(clustered_nodes)\n",
    "        key = cluster_key_result # key to identify dict of results\n",
    "        \n",
    "    else:\n",
    "        key = clusters_keys\n",
    "        cluster_info = clusters_infos[f'cluster_{key}']\n",
    "        # print(cluster_info)\n",
    "        incoming_pipes_ids.extend(cluster_info['incoming_pipes_id'])\n",
    "        incoming_pipes_ids_trout_nodes.extend(cluster_info['incoming_pipes_remixing_node_id'])\n",
    "        outgoing_pipes_ids.extend(cluster_info['outgoing_pipe_id'])\n",
    "        outgoing_pipes_ids_tsout_nodes.extend(cluster_info['outgoing_pipe_remixing_node_id'])\n",
    "        clustered_nodes.extend(cluster_info['cluster_nodes'])\n",
    "        associated_node_ids = generate_reduced_network_node_id_dict(cluster_info['cluster_nodes'])\n",
    "\n",
    "    perfs['key'] = key\n",
    "    perfs['type'] = cluster_info['cluster_type']\n",
    "    perfs['clustered_nodes_number'] = len(clustered_nodes)\n",
    "    perfs['clustered_nodes_percentage'] = 100*(len(clustered_nodes) / 71)\n",
    "    perfs['clustered_pipes_percentage'] = 100*((len(clustered_nodes)+len(outgoing_pipes_ids)) / 71) # Tree-like assumption => number of nodes == number edges\n",
    "    \n",
    "    loads_mare_f = []\n",
    "    \n",
    "    for i, el in enumerate(incoming_pipes_ids):\n",
    "        el_i = incoming_pipes_ids_trout_nodes[i]\n",
    "        pipe_index = int(el)-1 # Ingoing pipe index in the original network (needed for effective comparison) !! -1 as julia are indexed from 1\n",
    "        node_index = int(el_i)-1 # Adjacent nodes to the cluster (in the return direction) !! -1 as julia are indexed from 1\n",
    "\n",
    "        mw = np.abs(np.mean(scenario_dict_values['full_physic']['mw'][:,pipe_index]))\n",
    "        # print(f'Original incoming pipe (scenario) {el} traversed by mass flow rates with nominal value of {mw:.2f} Kg/s')\n",
    "\n",
    "        tr_node_mix = scenario_dict_values['full_physic']['tr'][:,node_index]\n",
    "        ts_node_mix = scenario_dict_values['full_physic']['ts'][:,node_index]\n",
    "            \n",
    "        tr_node_mix_reduced = scenario_dict_values[model_key][key]['tr'][:,associated_node_ids[node_index]]\n",
    "        ts_node_mix_reduced = scenario_dict_values[model_key][key]['ts'][:,associated_node_ids[node_index]]\n",
    "        \n",
    "        pw_adj_node = mw*4200*(ts_node_mix-tr_node_mix)\n",
    "        pw_adj_node_reduced = mw*4200*(ts_node_mix_reduced-tr_node_mix_reduced)\n",
    "        \n",
    "        loads_mare_f.extend(100*(np.abs(1 - pw_adj_node_reduced/pw_adj_node)))\n",
    "        \n",
    "    for i, el in enumerate(outgoing_pipes_ids):\n",
    "        el_i = outgoing_pipes_ids_tsout_nodes[i]\n",
    "        pipe_index = int(el)-1\n",
    "        node_index = int(el_i)-1\n",
    "        \n",
    "        mw = np.abs(np.mean(scenario_dict_values['full_physic']['mw'][:,pipe_index]))\n",
    "\n",
    "        tr_node_mix = scenario_dict_values['full_physic']['tr'][:,node_index]\n",
    "        ts_node_mix = scenario_dict_values['full_physic']['ts'][:,node_index]\n",
    "        tr_node_mix_reduced = scenario_dict_values[model_key][key]['tr'][:,associated_node_ids[node_index]]\n",
    "        ts_node_mix_reduced = scenario_dict_values[model_key][key]['ts'][:,associated_node_ids[node_index]]\n",
    "        \n",
    "        pw_adj_node = mw*4200*(ts_node_mix-tr_node_mix)\n",
    "        pw_adj_node_reduced = mw*4200*(ts_node_mix_reduced-tr_node_mix_reduced)\n",
    "        \n",
    "        loads_mare_f.extend(100*(np.abs(1 - pw_adj_node_reduced/pw_adj_node)))\n",
    "        \n",
    "\n",
    "    mare_f, conf_f = compute_mae_with_interval(loads_mare_f)\n",
    "    \n",
    "    perfs['Thermal loads (MARE)'] = mare_f\n",
    "    perfs['Thermal loads (MARE-Conf_interval)'] = conf_f\n",
    "    perfs['Thermal loads (errors)'] = loads_mare_f\n",
    "        \n",
    "    tr_node_source_f = scenario_dict_values['full_physic']['tr'][:,sources_to_consider].reshape(1,-1)\n",
    "    ts_node_source_f = scenario_dict_values['full_physic']['ts'][:,sources_to_consider].reshape(1,-1)\n",
    "    \n",
    "    associated_sources_nodes_ids = [associated_node_ids[i] for i in sources_to_consider]\n",
    "    tr_node_source_reduced_f = scenario_dict_values[model_key][key]['tr'][:,associated_sources_nodes_ids].reshape(1,-1)\n",
    "    ts_node_source_reduced_f = scenario_dict_values[model_key][key]['ts'][:,associated_sources_nodes_ids].reshape(1,-1)\n",
    "\n",
    "    mw_source_node_f = scenario_dict_values['full_physic']['mw'][:,sources_to_consider].reshape(1,-1)\n",
    "    \n",
    "    pw_adj_node_f = mw_source_node_f*4200*(ts_node_source_f-tr_node_source_f)\n",
    "    pw_adj_node_reduced_f = mw_source_node_f*4200*(ts_node_source_reduced_f-tr_node_source_reduced_f)\n",
    "    \n",
    "    mae_r_f, conf_r_f = compute_mae_with_interval(np.abs(tr_node_source_f-tr_node_source_reduced_f))\n",
    "    # print(f'Source return temperature conservation (scenario) MAE = {mae_r_f:.4f} °C +- {conf_r_f:.4f}') \n",
    "    perfs['Source return temp (MAE)'] = mae_r_f\n",
    "    perfs['Source return temp (MAE-Conf_interval)'] = conf_r_f\n",
    "    perfs['Source return temp (errors)'] = np.abs(tr_node_source_f-tr_node_source_reduced_f)\n",
    "\n",
    "    mare_pw_f, conf_pw_f = compute_mae_with_interval(100*np.abs(1 - pw_adj_node_reduced_f/pw_adj_node_f))\n",
    "    # print(f'Source generation power conservation (scenario) MARE = {mare_pw_f:.4f} % +- {conf_pw_f:.4f}') \n",
    "    perfs['Source gen (MARE)'] = mare_pw_f\n",
    "    perfs['Source gen (MARE-Conf_interval)'] = conf_pw_f\n",
    "    perfs['Source gen (errors)'] = 100*np.abs(1 - pw_adj_node_reduced_f/pw_adj_node_f)\n",
    "    \n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading results \n",
    "\n",
    "folder = os.path.join('ARTICLE_hybrid_results', 'third scenario')\n",
    "third_scenario_results = {}\n",
    "# Note: non present values are extrapolated\n",
    "# Les demandes ne sont pas modifiés trop dont utiliser l'autre \n",
    "# 1st scenario == ts var mais demandes differentes\n",
    "\n",
    "full_network_physical_file_path = os.path.join(folder, 'original_scenario_3.mat')\n",
    "third_scenario_results['full_physic'] = h5py.File(full_network_physical_file_path, 'r') # Dict ['load', 'mc', 'mw', 'topology_load', 'tr', 'trcs', 'trin', 'trout', 'ts', 'tsin', 'tsout']\n",
    "third_scenario_results['full_physic'] = prepocessing(third_scenario_results['full_physic'])\n",
    "for cluster_key in ['a', 'b', 'd', 'e', 'f', 'g', 'i', 'm', 'g_manual_1']:\n",
    "    for model in ['cnn', 'rnn']:\n",
    "        if model not in third_scenario_results:\n",
    "            third_scenario_results[model] = {}\n",
    "        reduced_file_path_rnn = os.path.join(folder, f'hybrid_{cluster_key}_scenario_3_model_{model}.mat')\n",
    "        if os.path.exists(reduced_file_path_rnn):\n",
    "            dict_values = h5py.File(reduced_file_path_rnn, 'r')\n",
    "            third_scenario_results[model][cluster_key] = prepocessing(dict_values)\n",
    "        else:\n",
    "            print(f'Cluster {cluster_key} not in 3rd scenario results for model {model}')\n",
    "\n",
    "sources = [0] # can add '56'\n",
    "perfs_per_models = {}\n",
    "for model in ['rnn', 'cnn']:\n",
    "    perfs_per_models[model] = {}\n",
    "    for key in ['a', 'b', 'd', 'e', 'f', 'i', 'g', 'm', 'g_manual_1']: # Note that in the article we do not consider combinaison clusters\n",
    "        if key in third_scenario_results[model]:\n",
    "            if key == 'g_manual_1':\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(third_scenario_results, model, ['g', 'manual_1'], cluster_key_result='g_manual_1', sources_to_consider=sources)\n",
    "            else:\n",
    "                perfs_per_models[model][key] = get_hybrid_simulation_performances(third_scenario_results, model, key, sources_to_consider=sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to pickle formats\n",
    "\n",
    "folder_scenario_1 = 'scenario_3_results'\n",
    "if not os.path.isdir(folder_scenario_1):\n",
    "    os.mkdir(folder_scenario_1)\n",
    "\n",
    "import json\n",
    "import pickle \n",
    "# Saving all considered clusters ['a', 'b', 'd', 'e', 'f', 'i', 'g', 'm']:\n",
    "for key in ['a', 'b', 'd', 'e', 'f', 'i', 'g', 'm']:\n",
    "    for model in ['rnn', 'cnn']:\n",
    "        try:\n",
    "            hybrid_results = perfs_per_models[model][key]\n",
    "            with open(os.path.join(folder_scenario_1, f'results_{model}_{key}.pkl'), 'wb') as f:\n",
    "                pickle.dump(hybrid_results, f)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "with open(os.path.join(folder_scenario_1, f'results_full_network.pkl'), 'wb') as f:\n",
    "    pickle.dump(third_scenario_results['full_physic'], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_clusters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
